{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4427179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2da9a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=100\n",
    "# Define the target function (the actual line)\n",
    "def f(x):\n",
    "    return np.sign(x[1] - x[0]+0.25)\n",
    "#generate the points\n",
    "X_train = np.random.uniform(-1, 1, (N, 2))\n",
    "y_train = np.array([f(x) for x in X_train])\n",
    "y_train_noise=y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5fd390",
   "metadata": {},
   "source": [
    "# flip 10% of the data to create noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d4042cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of elements to flip (10% of N)\n",
    "num_to_flip = int(0.1 * N)\n",
    "\n",
    "# Generate random indices to flip\n",
    "indices_to_flip = np.random.choice(N, num_to_flip, replace=False)\n",
    "\n",
    "# Modify the selected elements in y_train to another value (e.g., -1)\n",
    "y_train_noise[indices_to_flip] = -y_train_noise[indices_to_flip]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61601a7",
   "metadata": {},
   "source": [
    "# do the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7456fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_extended= np.column_stack((X_train, np.ones(N)))  # Add a bias term\n",
    "X_pseudo_inverse= np.linalg.pinv(X_extended)\n",
    "weights_bias = np.dot(X_pseudo_inverse, y_train_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2bf18",
   "metadata": {},
   "source": [
    "# calculate the in sample error after LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b2a94732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labes_in_sample=np.sign(np.dot(X_extended,weights_bias))\n",
    "np.sum(labes_in_sample!=y_train)/N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21332d9e",
   "metadata": {},
   "source": [
    "# do the non-linear transformation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7f846e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14\n",
      "[-0.73742096  0.84818747  0.3237972  -0.0987008   0.3237972   0.20742093]\n"
     ]
    }
   ],
   "source": [
    "N=1000\n",
    "# Define the target function (the actual line)\n",
    "def f(x):\n",
    "    return np.sign(x[1] - x[0]+0.25)\n",
    "#generate the points\n",
    "X_train = np.random.uniform(-1, 1, (N, 2))\n",
    "y_train = np.array([f(x) for x in X_train])\n",
    "y_train_noise=y_train\n",
    "# Determine the number of elements to flip (10% of N)\n",
    "num_to_flip = int(0.1 * N)\n",
    "\n",
    "# Generate random indices to flip\n",
    "indices_to_flip = np.random.choice(N, num_to_flip, replace=False)\n",
    "\n",
    "# Modify the selected elements in y_train to another value (e.g., -1)\n",
    "y_train_noise[indices_to_flip] = -y_train_noise[indices_to_flip]\n",
    "\n",
    "X_non_linear=np.zeros((N,5))\n",
    "\n",
    "X_non_linear[:,0]=X_train[:,0]\n",
    "X_non_linear[:,1]=X_train[:,1]\n",
    "X_non_linear[:,2]=X_train[:,0]*X_train[:,1]\n",
    "X_non_linear[:,3]=X_train[:,0]*X_train[:,0]\n",
    "X_non_linear[:,4]=X_train[:,0]*X_train[:,1]\n",
    "X_non_linear_extended=np.column_stack((X_non_linear, np.ones(N)))\n",
    "X_pseudo_inverse= np.linalg.pinv(X_non_linear_extended)\n",
    "weights_bias = np.dot(X_pseudo_inverse, y_train_noise)\n",
    "labes_in_sample=np.sign(np.dot(X_non_linear_extended,weights_bias))\n",
    "print(np.sum(labes_in_sample!=y_train)/N)\n",
    "\n",
    "print(weights_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "4f2b0a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Weight Vector: [-1.00865308  0.00813395 -0.02783007 -0.04117778  1.62081046  1.51142956]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the target function (the actual line)\n",
    "def f(x):\n",
    "    return np.sign(x[0]**2 + x[1]**2 - 0.6)\n",
    "\n",
    "N = 1000\n",
    "num_runs = 10\n",
    "\n",
    "# Initialize an array to store weight vectors for each run\n",
    "all_weights = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    # Generate the points\n",
    "    X_train = np.random.uniform(-1, 1, (N, 2))\n",
    "    y_train = np.array([f(x) for x in X_train])\n",
    "\n",
    "    # Add noise to labels (flip 10% of them)\n",
    "    num_to_flip = int(0.1 * N)\n",
    "    indices_to_flip = np.random.choice(N, num_to_flip, replace=False)\n",
    "    y_train[indices_to_flip] *= -1\n",
    "\n",
    "    # Transform the data using the feature vector (1, x1, x2, x1x2, x1^2, x2^2)\n",
    "    X_transformed = np.column_stack((np.ones(N), X_train[:, 0], X_train[:, 1], X_train[:, 0] * X_train[:, 1], X_train[:, 0]**2, X_train[:, 1]**2))\n",
    "\n",
    "    # Calculate the pseudo-inverse\n",
    "    X_pseudo_inverse = np.linalg.pinv(X_transformed)\n",
    "\n",
    "    # Calculate weights for Linear Regression\n",
    "    weights = np.dot(X_pseudo_inverse, y_train)\n",
    "    \n",
    "    # Append the weights for this run to the list\n",
    "    all_weights.append(weights)\n",
    "\n",
    "# Calculate the average weight vector over all runs\n",
    "average_weights = np.mean(all_weights, axis=0)\n",
    "print(\"Average Weight Vector:\", average_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "c690a36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Eout: 0.127631\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the target function (the actual line)\n",
    "def f(x):\n",
    "    return np.sign(x[0]**2 + x[1]**2 - 0.6)\n",
    "\n",
    "N = 1000\n",
    "num_runs = 1000  # Number of runs to estimate Eout\n",
    "Eout_values = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    # Generate a new set of points (out-of-sample data)\n",
    "    X_out = np.random.uniform(-1, 1, (N, 2))\n",
    "    y_out = np.array([f(x) for x in X_out])\n",
    "\n",
    "    # Add noise to labels (flip 10% of them)\n",
    "    num_to_flip = int(0.1 * N)\n",
    "    indices_to_flip = np.random.choice(N, num_to_flip, replace=False)\n",
    "    y_out[indices_to_flip] *= -1\n",
    "\n",
    "    # Transform the out-of-sample data using the same feature vector\n",
    "    X_out_transformed = np.column_stack((np.ones(N), X_out[:, 0], X_out[:, 1], X_out[:, 0] * X_out[:, 1], X_out[:, 0]**2, X_out[:, 1]**2))\n",
    "\n",
    "    # Calculate out-of-sample classification error Eout\n",
    "    Eout = np.mean(y_out != np.sign(X_out_transformed.dot(average_weights)))\n",
    "    Eout_values.append(Eout)\n",
    "\n",
    "# Calculate the average out-of-sample error Eout over 1000 runs\n",
    "average_Eout = np.mean(Eout_values)\n",
    "print(\"Average Eout:\", average_Eout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dfb639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
